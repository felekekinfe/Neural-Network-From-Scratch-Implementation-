```markdown
# Neural Network From Scratch (NumPy)

A minimal, **from-scratch implementation of a feedforward neural network** using only **NumPy**.  
This project demonstrates how core deep learning components work *under the hood* without relying on high-level frameworks like TensorFlow or PyTorch.

The model is trained on a **2D spiral dataset** for multi-class classification.

---

## ðŸ“Œ Features

- Fully connected (Dense) layers
- ReLU activation
- Softmax activation
- Categorical Cross-Entropy loss
- Optimized **Softmax + Cross-Entropy** backward pass
- Stochastic Gradient Descent (SGD) optimizer
- End-to-end forward & backward propagation
- Training loop with accuracy and loss logging

---

## ðŸ§  Model Architecture

```

Input (2)
â†“
Dense (2 â†’ 64)
â†“
ReLU
â†“
Dense (64 â†’ 3)
â†“
Softmax + Categorical Cross-Entropy

```

---

## ðŸ§® Mathematical Formulation

**Dense layer**
```

Z = XW + b

```

**ReLU**
```

ReLU(x) = max(0, x)

```

**Softmax**
```

Softmax(z_i) = exp(z_i) / Î£ exp(z_j)

```

**Categorical Cross-Entropy Loss**
```

L = -log(p_correct)

```

**Optimized Gradient (Softmax + Cross-Entropy)**
```

âˆ‚L/âˆ‚z = (y_pred - y_true) / N

````

---

## ðŸš€ Getting Started

### 1. Install Dependencies
```bash
pip install numpy nnfs
````

### 2. Run the Script

```bash
python main.py
```

---

## ðŸ“Š Training Output

The training loop runs for **10,001 epochs** and prints progress every 1000 epochs:

```
epoch: 0, acc: 0.340, loss: 1.099
epoch: 1000, acc: 0.810, loss: 0.465
epoch: 10000, acc: 0.950, loss: 0.120
```

---

## ðŸ§© Code Structure

* `Layer_Dense` â€“ Fully connected layer
* `Activation_ReLU` â€“ Non-linear activation
* `Activation_Softmax` â€“ Output activation
* `Loss_CategoricalCrossentropy` â€“ Classification loss
* `Activation_Softmax_Loss_CategoricalCrossentropy` â€“ Optimized combined layer
* `Optimizer_SGD` â€“ Parameter update rule
* Training loop â€“ Forward pass â†’ Backward pass â†’ Update

---

## ðŸŽ¯ Learning Objectives

This project is ideal if you want to:

* Understand **backpropagation mathematically**
* Learn how gradients flow through a network
* See how optimizations like **Softmax + CE fusion** work
* Build intuition before using deep learning frameworks

---

## ðŸ“š Inspiration

Inspired by *Neural Networks from Scratch* (NNFS) by Harrison Kinsley & Daniel KukieÅ‚a.

---

## ðŸ“„ License

MIT License â€“ free to use, modify, and learn from.

---

**Built for learning, not abstraction.**

```
```
